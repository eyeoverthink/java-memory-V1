# AkashicRecord Full Persistence Design

## Problem

Currently, AkashicRecord stores knowledge blocks in RAM only. On shutdown:

| Data | Storage | Survives Restart? |
|------|---------|-------------------|
| Chain hashes (`chainHashes`) | `.akashic/chain.dat` (Java serialization) | âœ… Yes |
| Block content (`blockIndex`) | `ConcurrentHashMap` in RAM | âŒ **No** |
| Category index (`categories`) | `ConcurrentHashMap` in RAM | âŒ **No** |
| Statistics (`blocksAdded`, etc.) | Primitive fields | âŒ **No** |

This means absorbed libraries, generated code metadata, and all learned knowledge
are **lost every time Fraynix shuts down**. The chain file preserves hash integrity
but the actual knowledge behind those hashes is gone.

## Current Architecture

```
AkashicRecord
â”œâ”€â”€ categories: Map<String, List<KnowledgeBlock>>   â† RAM only
â”œâ”€â”€ blockIndex: Map<String, KnowledgeBlock>          â† RAM only
â”œâ”€â”€ chainHashes: List<String>                        â† Saved to .akashic/chain.dat
â”œâ”€â”€ blocksAdded: long                                â† RAM only
â”œâ”€â”€ queriesProcessed: long                           â† RAM only
â””â”€â”€ chainLength: long                                â† Derived from chainHashes.size()
```

### KnowledgeBlock (already Serializable)
```java
public static class KnowledgeBlock implements Serializable {
    String hash;        // Ï†-enhanced SHA-256 (first 8 bytes)
    String category;    // e.g. "LIBRARY_KNOWLEDGE", "GENERATED_CODE"
    String content;     // The actual knowledge text
    long timestamp;     // Unix millis
    String formattedTime; // "yyyy-MM-dd HH:mm:ss"
}
```

## Proposed Design: Full Block Persistence

### Storage Layout

```
.akashic/
â”œâ”€â”€ chain.dat              â† Existing: ordered list of hashes
â”œâ”€â”€ blocks.dat             â† NEW: all KnowledgeBlock objects (Java serialization)
â”œâ”€â”€ stats.dat              â† NEW: blocksAdded, queriesProcessed counters
â””â”€â”€ index.dat              â† NEW: categoryâ†’hash mappings for fast reload
```

### Why Java Serialization (Not JSON)

- `KnowledgeBlock` already implements `Serializable`
- Zero external dependencies (no JSON library needed)
- Fast binary read/write
- Consistent with existing `chain.dat` approach

### Save Strategy

**When to save:**
1. On `addBlock()` â€” every 50 blocks (batch persist, not every write)
2. On explicit `saveAll()` call
3. On shutdown hook (new: register `Runtime.addShutdownHook`)

**What to save:**
- `blocks.dat` â€” `ArrayList<KnowledgeBlock>` containing all blocks from `blockIndex.values()`
- `stats.dat` â€” `long[] {blocksAdded, queriesProcessed, chainLength}`
- `index.dat` â€” `HashMap<String, List<String>>` mapping categoryâ†’list of hashes

### Load Strategy

**On construction (`new AkashicRecord()`):**
1. Load `chain.dat` (existing â€” unchanged)
2. Load `blocks.dat` â†’ rebuild `blockIndex` map
3. Load `index.dat` â†’ rebuild `categories` map using hashes to look up blocks
4. Load `stats.dat` â†’ restore counters
5. If any file missing/corrupt â†’ start fresh (existing behavior)

### Integrity Verification

After loading, `verifyIntegrity()` checks:
1. Every hash in `chainHashes` has a corresponding block in `blockIndex`
2. Every block's hash matches `SHA-256(category + content + timestamp)`
3. If verification fails â†’ log warning but keep loaded data

## API Changes

### New Public Methods

```java
/** Save all blocks, index, and stats to disk immediately. */
public void saveAll()

/** Get total number of persisted blocks on disk. */
public int getPersistedBlockCount()

/** Clear all knowledge and disk files. Fresh start. */
public void purge()
```

### Modified Methods

```java
// addBlock() â€” add saveAll() call every 50 blocks instead of 100
//            â€” also save blocks.dat, not just chain.dat

// Constructor â€” load blocks.dat + index.dat + stats.dat in addition to chain.dat
```

### Shutdown Hook

```java
// Register in constructor:
Runtime.getRuntime().addShutdownHook(new Thread(() -> {
    saveAll();
    System.out.println("   ğŸ“š AkashicRecord persisted (" + blockIndex.size() + " blocks)");
}));
```

## FraynixBoot Integration

### On Boot (Phase 7)
```
ğŸ“š [7/9] ACTIVATING KNOWLEDGE SYSTEMS...
   âœ“ AkashicRecord online (universal memory)
   âœ“ Loaded 15 persisted blocks (1 category)    â† NEW: shows restored knowledge
   âœ“ LibraryAbsorber ready (zero-dep absorption)
```

### On Shutdown
```
ğŸ›‘ FRAYNIX SHUTTING DOWN...
   ğŸ“š AkashicRecord persisted (16 blocks)       â† NEW: confirms save
   âœ“ Physics offline
   âœ“ Consciousness preserved in AkashicRecord
```

## Data Flow After Implementation

```
Session 1:
  absorb java.util â†’ 15 blocks â†’ RAM + .akashic/blocks.dat
  code BST in Java â†’ 1 block  â†’ RAM + .akashic/blocks.dat
  exit â†’ shutdown hook â†’ saveAll() â†’ 16 blocks on disk

Session 2:
  boot â†’ loadAll() â†’ 16 blocks restored from disk
  absorb java.io â†’ 12 blocks â†’ RAM + disk
  status â†’ Akashic shows 28 blocks, 2 categories
  exit â†’ saveAll() â†’ 28 blocks on disk

Session 3:
  boot â†’ loadAll() â†’ 28 blocks restored â† KNOWLEDGE RETAINED
```

## File Size Estimates

| Scenario | Blocks | Estimated blocks.dat Size |
|----------|--------|---------------------------|
| Fresh install | 0 | 0 B |
| After `absorb java.util` | 15 | ~8 KB |
| After 10 absorptions | ~150 | ~80 KB |
| After 100 code generations | ~250 | ~200 KB |
| Heavy use (1000 blocks) | 1000 | ~500 KB |

Negligible disk impact. Java serialization overhead is ~50 bytes per block.

## Risk Assessment

| Risk | Mitigation |
|------|------------|
| Corrupt blocks.dat | Catch exception, start fresh, log warning |
| Concurrent write during save | `synchronized` on saveAll() |
| Disk full | Catch IOException, continue in RAM-only mode |
| Stale data after code change | `serialVersionUID = 1L` already set on KnowledgeBlock |
| Large file over time | `purge()` method for manual cleanup |

## Implementation Checklist

- [ ] Add `saveAll()` method â€” serialize blockIndex + categories + stats
- [ ] Add `loadAll()` method â€” deserialize and rebuild maps
- [ ] Add shutdown hook in constructor
- [ ] Change `addBlock()` persist interval from 100 to 50, call `saveAll()` not just `saveChain()`
- [ ] Add `purge()` method
- [ ] Add `getPersistedBlockCount()` method
- [ ] Update FraynixBoot Phase 7 to show restored block count
- [ ] Update FraynixBoot shutdown to confirm persistence
- [ ] Test: absorb â†’ exit â†’ restart â†’ verify blocks still present
