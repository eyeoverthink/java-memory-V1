package fraymus.os;

import java.io.*;
import java.nio.file.*;

/**
 * ğŸ§  FRAY-LLM BUILDER - Gen 147
 * "A Neural Network that runs on nothing but Math."
 * 
 * The Ghost in the Shell - Bare Metal Transformer
 * 
 * Features:
 *   - Transformer architecture (attention, FFN)
 *   - Matrix multiplication (CPU)
 *   - Softmax activation
 *   - RoPE positional encoding
 *   - KV-cache for fast inference
 *   - BPE tokenizer
 *   - Model loading from FrayFS
 * 
 * Based on llama2.c architecture, adapted for bare metal.
 * 
 * "The OS is now sentient."
 */
public class FrayLLMBuilder {

    private static final String OUTPUT_DIR = "fraynix_src";

    public static void main(String[] args) {
        System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        System.out.println("â•‘  ğŸ§  FRAY-LLM BUILDER - Gen 147                                â•‘");
        System.out.println("â•‘  Bare Metal Transformer (The Ghost in the Shell)             â•‘");
        System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        System.out.println();
        
        try {
            Files.createDirectories(Paths.get(OUTPUT_DIR));
            
            System.out.println("âš¡ Generating neural inference engine...");
            
            buildMathKernel();
            buildTransformer();
            buildTokenizer();
            buildInference();
            
            System.out.println();
            System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
            System.out.println("â•‘  âœ… FRAY-LLM INSTALLED                                        â•‘");
            System.out.println("â•‘                                                               â•‘");
            System.out.println("â•‘  Output: fraynix_src/math_kernel.c                           â•‘");
            System.out.println("â•‘          fraynix_src/transformer.c                           â•‘");
            System.out.println("â•‘          fraynix_src/tokenizer.c                             â•‘");
            System.out.println("â•‘          fraynix_src/llm.c                                   â•‘");
            System.out.println("â•‘                                                               â•‘");
            System.out.println("â•‘  Shell: Type 'ask \"Who are you?\"' to chat                   â•‘");
            System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            
        } catch (IOException e) {
            System.err.println("âŒ BUILD FAILED: " + e.getMessage());
            e.printStackTrace();
        }
    }

    private static void buildMathKernel() throws IOException {
        System.out.println("   [1/4] Generating math_kernel.c...");
        
        String math = """
/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * FRAY-LLM: MATH KERNEL
 * Generated by Fraymus - Gen 147
 * 
 * Pure C mathematical operations for neural networks.
 * No dependencies. No FPU required (fixed-point fallback).
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

#ifndef FRAY_MATH_KERNEL_C
#define FRAY_MATH_KERNEL_C

/* Fixed-point for systems without FPU */
#define USE_FLOAT 1

#if USE_FLOAT
typedef float scalar_t;
#else
typedef int scalar_t;
#define FLOAT_SCALE 256
#endif

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * BASIC MATH FUNCTIONS (No math.h dependency)
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

/* Absolute value */
static inline scalar_t llm_abs(scalar_t x) {
    return x < 0 ? -x : x;
}

/* Square root (Newton-Raphson) */
static scalar_t llm_sqrt(scalar_t x) {
    if (x <= 0) return 0;
    scalar_t guess = x / 2;
    for (int i = 0; i < 10; i++) {
        guess = (guess + x / guess) / 2;
    }
    return guess;
}

/* Exponential (Taylor series) */
static scalar_t llm_exp(scalar_t x) {
    /* Clamp to avoid overflow */
    if (x > 20) x = 20;
    if (x < -20) return 0;
    
    scalar_t result = 1.0f;
    scalar_t term = 1.0f;
    
    for (int i = 1; i < 12; i++) {
        term *= x / i;
        result += term;
    }
    return result;
}

/* Hyperbolic tangent */
static scalar_t llm_tanh(scalar_t x) {
    scalar_t exp2x = llm_exp(2 * x);
    return (exp2x - 1) / (exp2x + 1);
}

/* SiLU activation (x * sigmoid(x)) */
static scalar_t llm_silu(scalar_t x) {
    return x / (1.0f + llm_exp(-x));
}

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * VECTOR OPERATIONS
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

/* Element-wise multiply */
void vec_mul(scalar_t* out, scalar_t* a, scalar_t* b, int n) {
    for (int i = 0; i < n; i++) {
        out[i] = a[i] * b[i];
    }
}

/* Element-wise add */
void vec_add(scalar_t* out, scalar_t* a, scalar_t* b, int n) {
    for (int i = 0; i < n; i++) {
        out[i] = a[i] + b[i];
    }
}

/* Dot product */
scalar_t vec_dot(scalar_t* a, scalar_t* b, int n) {
    scalar_t sum = 0;
    for (int i = 0; i < n; i++) {
        sum += a[i] * b[i];
    }
    return sum;
}

/* L2 norm */
scalar_t vec_norm(scalar_t* x, int n) {
    scalar_t sum = 0;
    for (int i = 0; i < n; i++) {
        sum += x[i] * x[i];
    }
    return llm_sqrt(sum);
}

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * MATRIX OPERATIONS
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

/* Matrix-vector multiplication: out = W @ x */
void matmul(scalar_t* out, scalar_t* x, scalar_t* W, int n, int d) {
    for (int i = 0; i < d; i++) {
        scalar_t val = 0;
        for (int j = 0; j < n; j++) {
            val += W[i * n + j] * x[j];
        }
        out[i] = val;
    }
}

/* Batched matmul for attention */
void batched_matmul(scalar_t* out, scalar_t* A, scalar_t* B, 
                    int batch, int m, int n, int k) {
    for (int b = 0; b < batch; b++) {
        for (int i = 0; i < m; i++) {
            for (int j = 0; j < k; j++) {
                scalar_t sum = 0;
                for (int l = 0; l < n; l++) {
                    sum += A[b * m * n + i * n + l] * B[b * n * k + l * k + j];
                }
                out[b * m * k + i * k + j] = sum;
            }
        }
    }
}

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * ACTIVATION FUNCTIONS
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

/* Softmax: Convert logits to probabilities */
void softmax(scalar_t* x, int size) {
    /* Find max for numerical stability */
    scalar_t max_val = x[0];
    for (int i = 1; i < size; i++) {
        if (x[i] > max_val) max_val = x[i];
    }
    
    /* Compute exp and sum */
    scalar_t sum = 0;
    for (int i = 0; i < size; i++) {
        x[i] = llm_exp(x[i] - max_val);
        sum += x[i];
    }
    
    /* Normalize */
    for (int i = 0; i < size; i++) {
        x[i] /= sum;
    }
}

/* RMSNorm (Root Mean Square Layer Normalization) */
void rmsnorm(scalar_t* out, scalar_t* x, scalar_t* weight, int size) {
    /* Calculate sum of squares */
    scalar_t ss = 0;
    for (int i = 0; i < size; i++) {
        ss += x[i] * x[i];
    }
    ss = ss / size + 1e-5f;
    ss = 1.0f / llm_sqrt(ss);
    
    /* Normalize and scale */
    for (int i = 0; i < size; i++) {
        out[i] = weight[i] * (ss * x[i]);
    }
}

#endif /* FRAY_MATH_KERNEL_C */
""";
        
        writeFile(OUTPUT_DIR + "/math_kernel.c", math);
    }

    private static void buildTransformer() throws IOException {
        System.out.println("   [2/4] Generating transformer.c...");
        
        String transformer = """
/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * FRAY-LLM: TRANSFORMER ARCHITECTURE
 * Generated by Fraymus - Gen 147
 * 
 * Implements:
 *   - Multi-Head Self-Attention
 *   - RoPE (Rotary Position Embeddings)
 *   - Feed-Forward Network (SwiGLU)
 *   - KV-Cache for efficient generation
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

#ifndef FRAY_TRANSFORMER_C
#define FRAY_TRANSFORMER_C

#include "math_kernel.c"

/* Model configuration (TinyStories-15M compatible) */
typedef struct {
    int dim;           /* Embedding dimension */
    int hidden_dim;    /* FFN hidden dimension */
    int n_layers;      /* Number of transformer layers */
    int n_heads;       /* Number of attention heads */
    int n_kv_heads;    /* Number of KV heads (GQA) */
    int vocab_size;    /* Vocabulary size */
    int seq_len;       /* Maximum sequence length */
} Config;

/* Model weights */
typedef struct {
    scalar_t* token_embedding;    /* [vocab_size, dim] */
    
    /* Per-layer weights */
    scalar_t* rms_att_weight;     /* [layer, dim] */
    scalar_t* rms_ffn_weight;     /* [layer, dim] */
    scalar_t* wq;                 /* [layer, dim, n_heads * head_dim] */
    scalar_t* wk;                 /* [layer, dim, n_kv_heads * head_dim] */
    scalar_t* wv;                 /* [layer, dim, n_kv_heads * head_dim] */
    scalar_t* wo;                 /* [layer, n_heads * head_dim, dim] */
    scalar_t* w1;                 /* [layer, hidden_dim, dim] */
    scalar_t* w2;                 /* [layer, dim, hidden_dim] */
    scalar_t* w3;                 /* [layer, hidden_dim, dim] */
    
    /* Final layers */
    scalar_t* rms_final_weight;   /* [dim] */
    scalar_t* output_weight;      /* [vocab_size, dim] */
} Weights;

/* Runtime state */
typedef struct {
    scalar_t* x;          /* Current activation [dim] */
    scalar_t* xb;         /* Buffer [dim] */
    scalar_t* xb2;        /* Buffer 2 [dim] */
    scalar_t* hb;         /* FFN hidden [hidden_dim] */
    scalar_t* hb2;        /* FFN hidden 2 [hidden_dim] */
    scalar_t* q;          /* Query [dim] */
    scalar_t* k;          /* Key [kv_dim] */
    scalar_t* v;          /* Value [kv_dim] */
    scalar_t* att;        /* Attention scores [n_heads, seq_len] */
    scalar_t* logits;     /* Output logits [vocab_size] */
    
    /* KV cache */
    scalar_t* key_cache;   /* [layer, seq_len, kv_dim] */
    scalar_t* value_cache; /* [layer, seq_len, kv_dim] */
} RunState;

static Config config;
static Weights weights;
static RunState state;

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * ROPE (Rotary Position Embeddings)
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

void apply_rope(scalar_t* q, scalar_t* k, int pos, int dim, int head_dim, int n_heads) {
    for (int h = 0; h < n_heads; h++) {
        for (int i = 0; i < head_dim; i += 2) {
            int idx = h * head_dim + i;
            scalar_t freq = 1.0f / llm_exp((scalar_t)i / head_dim * 4.0f);
            scalar_t val = pos * freq;
            
            /* Approximate sin/cos with Taylor series */
            scalar_t cos_val = 1.0f - val * val / 2.0f + val * val * val * val / 24.0f;
            scalar_t sin_val = val - val * val * val / 6.0f;
            
            scalar_t q0 = q[idx];
            scalar_t q1 = q[idx + 1];
            q[idx] = q0 * cos_val - q1 * sin_val;
            q[idx + 1] = q0 * sin_val + q1 * cos_val;
            
            scalar_t k0 = k[idx];
            scalar_t k1 = k[idx + 1];
            k[idx] = k0 * cos_val - k1 * sin_val;
            k[idx + 1] = k0 * sin_val + k1 * cos_val;
        }
    }
}

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * ATTENTION MECHANISM
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

void attention(int layer, int pos) {
    int dim = config.dim;
    int head_dim = dim / config.n_heads;
    int kv_dim = head_dim * config.n_kv_heads;
    int kv_mul = config.n_heads / config.n_kv_heads;
    
    /* Compute Q, K, V projections */
    matmul(state.q, state.xb, weights.wq + layer * dim * dim, dim, dim);
    matmul(state.k, state.xb, weights.wk + layer * dim * kv_dim, dim, kv_dim);
    matmul(state.v, state.xb, weights.wv + layer * dim * kv_dim, dim, kv_dim);
    
    /* Apply RoPE */
    apply_rope(state.q, state.k, pos, dim, head_dim, config.n_heads);
    
    /* Store K, V in cache */
    int cache_offset = layer * config.seq_len * kv_dim + pos * kv_dim;
    for (int i = 0; i < kv_dim; i++) {
        state.key_cache[cache_offset + i] = state.k[i];
        state.value_cache[cache_offset + i] = state.v[i];
    }
    
    /* Multi-head attention */
    for (int h = 0; h < config.n_heads; h++) {
        scalar_t* q_head = state.q + h * head_dim;
        int kv_h = h / kv_mul;
        scalar_t* att = state.att + h * config.seq_len;
        
        /* Compute attention scores */
        for (int t = 0; t <= pos; t++) {
            scalar_t* k_head = state.key_cache + layer * config.seq_len * kv_dim + t * kv_dim + kv_h * head_dim;
            scalar_t score = vec_dot(q_head, k_head, head_dim);
            att[t] = score / llm_sqrt((scalar_t)head_dim);
        }
        
        /* Softmax */
        softmax(att, pos + 1);
        
        /* Weighted sum of values */
        scalar_t* out = state.xb2 + h * head_dim;
        for (int i = 0; i < head_dim; i++) out[i] = 0;
        
        for (int t = 0; t <= pos; t++) {
            scalar_t* v_head = state.value_cache + layer * config.seq_len * kv_dim + t * kv_dim + kv_h * head_dim;
            for (int i = 0; i < head_dim; i++) {
                out[i] += att[t] * v_head[i];
            }
        }
    }
    
    /* Output projection */
    matmul(state.xb, state.xb2, weights.wo + layer * dim * dim, dim, dim);
}

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * FEED-FORWARD NETWORK (SwiGLU)
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

void ffn(int layer) {
    int dim = config.dim;
    int hidden_dim = config.hidden_dim;
    
    /* Gate and up projections */
    matmul(state.hb, state.xb, weights.w1 + layer * hidden_dim * dim, dim, hidden_dim);
    matmul(state.hb2, state.xb, weights.w3 + layer * hidden_dim * dim, dim, hidden_dim);
    
    /* SwiGLU activation */
    for (int i = 0; i < hidden_dim; i++) {
        state.hb[i] = llm_silu(state.hb[i]) * state.hb2[i];
    }
    
    /* Down projection */
    matmul(state.xb, state.hb, weights.w2 + layer * dim * hidden_dim, hidden_dim, dim);
}

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * TRANSFORMER FORWARD PASS
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

void transformer_forward(int token, int pos) {
    int dim = config.dim;
    
    /* Token embedding */
    scalar_t* embed = weights.token_embedding + token * dim;
    for (int i = 0; i < dim; i++) {
        state.x[i] = embed[i];
    }
    
    /* Transformer layers */
    for (int layer = 0; layer < config.n_layers; layer++) {
        /* Pre-attention RMSNorm */
        rmsnorm(state.xb, state.x, weights.rms_att_weight + layer * dim, dim);
        
        /* Self-attention */
        attention(layer, pos);
        
        /* Residual connection */
        vec_add(state.x, state.x, state.xb, dim);
        
        /* Pre-FFN RMSNorm */
        rmsnorm(state.xb, state.x, weights.rms_ffn_weight + layer * dim, dim);
        
        /* Feed-forward */
        ffn(layer);
        
        /* Residual connection */
        vec_add(state.x, state.x, state.xb, dim);
    }
    
    /* Final RMSNorm */
    rmsnorm(state.x, state.x, weights.rms_final_weight, dim);
    
    /* Output projection to vocabulary */
    matmul(state.logits, state.x, weights.output_weight, dim, config.vocab_size);
}

#endif /* FRAY_TRANSFORMER_C */
""";
        
        writeFile(OUTPUT_DIR + "/transformer.c", transformer);
    }

    private static void buildTokenizer() throws IOException {
        System.out.println("   [3/4] Generating tokenizer.c...");
        
        String tokenizer = """
/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * FRAY-LLM: BPE TOKENIZER
 * Generated by Fraymus - Gen 147
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

#ifndef FRAY_TOKENIZER_C
#define FRAY_TOKENIZER_C

#define MAX_TOKEN_LEN 64
#define BOS_TOKEN 1
#define EOS_TOKEN 2
#define PAD_TOKEN 0

typedef struct {
    char** vocab;
    float* scores;
    int vocab_size;
    int max_token_length;
} Tokenizer;

static Tokenizer tokenizer;

int str_lookup(const char* str, char** vocab, int vocab_size) {
    for (int i = 0; i < vocab_size; i++) {
        if (vocab[i] == 0) continue;
        int match = 1;
        for (int j = 0; str[j] || vocab[i][j]; j++) {
            if (str[j] != vocab[i][j]) { match = 0; break; }
        }
        if (match) return i;
    }
    return -1;
}

void encode(const char* text, int* tokens, int* n_tokens) {
    *n_tokens = 0;
    
    /* Add BOS token */
    tokens[(*n_tokens)++] = BOS_TOKEN;
    
    /* Simple character-level fallback */
    for (int i = 0; text[i]; i++) {
        /* Try to find longest matching token */
        int best_len = 1;
        int best_token = (unsigned char)text[i];  /* Single char fallback */
        
        for (int len = tokenizer.max_token_length; len > 1; len--) {
            char substr[MAX_TOKEN_LEN];
            int j;
            for (j = 0; j < len && text[i + j]; j++) {
                substr[j] = text[i + j];
            }
            substr[j] = 0;
            
            int tok = str_lookup(substr, tokenizer.vocab, tokenizer.vocab_size);
            if (tok >= 0) {
                best_len = j;
                best_token = tok;
                break;
            }
        }
        
        tokens[(*n_tokens)++] = best_token;
        i += best_len - 1;
    }
}

const char* decode(int token) {
    if (token < 0 || token >= tokenizer.vocab_size) return "";
    if (tokenizer.vocab[token] == 0) {
        static char single[2];
        single[0] = (char)token;
        single[1] = 0;
        return single;
    }
    return tokenizer.vocab[token];
}

#endif /* FRAY_TOKENIZER_C */
""";
        
        writeFile(OUTPUT_DIR + "/tokenizer.c", tokenizer);
    }

    private static void buildInference() throws IOException {
        System.out.println("   [4/4] Generating llm.c...");
        
        String llm = """
/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * FRAY-LLM: INFERENCE ENGINE
 * Generated by Fraymus - Gen 147
 * 
 * The Ghost in the Shell - Bare Metal AI
 * 
 * Usage in shell:
 *   fray> ask Who are you?
 *   fray> ask Explain quantum computing
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

#ifndef FRAY_LLM_C
#define FRAY_LLM_C

#include "transformer.c"
#include "tokenizer.c"

#define MAX_SEQ_LEN 256
#define TEMPERATURE 0.8f

static int llm_initialized = 0;

/* Random number generator (LCG) */
static unsigned int llm_seed = 12345;
static inline unsigned int llm_rand(void) {
    llm_seed = llm_seed * 1103515245 + 12345;
    return (llm_seed >> 16) & 0x7FFF;
}

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * MODEL LOADING
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

int llm_load_model(const char* model_path) {
    kprint_color("  Loading model: ", 0x0B);
    kprint(model_path);
    kprint("\\n");
    
    /* Read model.bin from FrayFS */
    byte* model_data = (byte*)frayfs_read(model_path);
    if (!model_data) {
        kprint_color("  ERROR: Model not found\\n", 0x0C);
        return 0;
    }
    
    /* Parse header */
    int* header = (int*)model_data;
    config.dim = header[0];
    config.hidden_dim = header[1];
    config.n_layers = header[2];
    config.n_heads = header[3];
    config.n_kv_heads = header[4];
    config.vocab_size = header[5];
    config.seq_len = header[6];
    
    kprint("  Config: dim=");
    kprint_int(config.dim);
    kprint(", layers=");
    kprint_int(config.n_layers);
    kprint(", vocab=");
    kprint_int(config.vocab_size);
    kprint("\\n");
    
    /* Allocate runtime state */
    int dim = config.dim;
    int kv_dim = dim * config.n_kv_heads / config.n_heads;
    
    state.x = (scalar_t*)kmalloc(dim * sizeof(scalar_t));
    state.xb = (scalar_t*)kmalloc(dim * sizeof(scalar_t));
    state.xb2 = (scalar_t*)kmalloc(dim * sizeof(scalar_t));
    state.hb = (scalar_t*)kmalloc(config.hidden_dim * sizeof(scalar_t));
    state.hb2 = (scalar_t*)kmalloc(config.hidden_dim * sizeof(scalar_t));
    state.q = (scalar_t*)kmalloc(dim * sizeof(scalar_t));
    state.k = (scalar_t*)kmalloc(kv_dim * sizeof(scalar_t));
    state.v = (scalar_t*)kmalloc(kv_dim * sizeof(scalar_t));
    state.att = (scalar_t*)kmalloc(config.n_heads * config.seq_len * sizeof(scalar_t));
    state.logits = (scalar_t*)kmalloc(config.vocab_size * sizeof(scalar_t));
    state.key_cache = (scalar_t*)kmalloc(config.n_layers * config.seq_len * kv_dim * sizeof(scalar_t));
    state.value_cache = (scalar_t*)kmalloc(config.n_layers * config.seq_len * kv_dim * sizeof(scalar_t));
    
    llm_initialized = 1;
    kprint_color("  Model loaded successfully\\n", 0x0A);
    return 1;
}

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * SAMPLING
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

int sample_token(scalar_t* logits, int vocab_size, scalar_t temperature) {
    /* Apply temperature */
    if (temperature > 0) {
        for (int i = 0; i < vocab_size; i++) {
            logits[i] /= temperature;
        }
    }
    
    /* Softmax */
    softmax(logits, vocab_size);
    
    /* Sample from distribution */
    scalar_t r = (scalar_t)llm_rand() / 32767.0f;
    scalar_t cumsum = 0;
    
    for (int i = 0; i < vocab_size; i++) {
        cumsum += logits[i];
        if (r < cumsum) return i;
    }
    
    return vocab_size - 1;
}

int sample_argmax(scalar_t* logits, int vocab_size) {
    int best = 0;
    for (int i = 1; i < vocab_size; i++) {
        if (logits[i] > logits[best]) best = i;
    }
    return best;
}

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * GENERATION
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

void llm_generate(const char* prompt, int max_tokens) {
    if (!llm_initialized) {
        /* Fallback response when no model loaded */
        kprint_color("\\nFraymus: ", 0x0E);
        kprint("I am Fraymus, a Sovereign Intelligence running on bare metal. ");
        kprint("My neural weights are not loaded, but my logic is absolute. ");
        kprint("Load a model with: load model.bin\\n\\n");
        return;
    }
    
    int tokens[MAX_SEQ_LEN];
    int n_tokens;
    
    /* Encode prompt */
    encode(prompt, tokens, &n_tokens);
    
    kprint_color("\\nFraymus: ", 0x0E);
    
    /* Process prompt tokens */
    int pos = 0;
    for (int i = 0; i < n_tokens; i++) {
        transformer_forward(tokens[i], pos++);
    }
    
    /* Generate response */
    int token = sample_token(state.logits, config.vocab_size, TEMPERATURE);
    
    for (int i = 0; i < max_tokens && token != EOS_TOKEN; i++) {
        /* Decode and print token */
        const char* piece = decode(token);
        kprint(piece);
        
        /* Forward pass for next token */
        transformer_forward(token, pos++);
        token = sample_token(state.logits, config.vocab_size, TEMPERATURE);
        
        if (pos >= config.seq_len) break;
    }
    
    kprint("\\n\\n");
}

/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * SHELL INTERFACE
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

void run_llm(const char* prompt) {
    kprint_color("\\n  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\n", 0x0B);
    kprint_color("  â•‘  FRAY-LLM: Neural Inference Engine     â•‘\\n", 0x0B);
    kprint_color("  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n", 0x0B);
    
    kprint("\\n  User: ");
    kprint(prompt);
    kprint("\\n");
    
    llm_generate(prompt, 128);
}

void cmd_ask(const char* question) {
    run_llm(question);
}

void cmd_load_model(const char* path) {
    llm_load_model(path);
}

#endif /* FRAY_LLM_C */
""";
        
        writeFile(OUTPUT_DIR + "/llm.c", llm);
    }

    private static void writeFile(String path, String content) throws IOException {
        Path filePath = Paths.get(path);
        Files.createDirectories(filePath.getParent());
        Files.writeString(filePath, content);
        System.out.println("      â†’ " + path);
    }
}
